{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In this file you will find the most current tools used to clasify an english text , using multiple classes \n",
    "#and with labeled dataset\n",
    "\n",
    "#Models used here is machine learning suppervised algorithms, \n",
    "#using the one-against-all method \"OAA\" to generalize to multiclass (instead of using a binary algorithm): \n",
    "\n",
    "#####1: Multinomial Naive Bayes\n",
    "#####2: Logistic regression\n",
    "#####3: Random Forest\n",
    "#####4: linear Support Vectors Machine\n",
    "#####5: Knearest nighbors\n",
    "\n",
    "#####6: we also will try Support Vectors Machine with the stochastic gradient descent \"SGD\" method to generalize to multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-  uploading file to a dataframe\n",
    "# uploading a csv file \n",
    "\n",
    "import pandas as pd # we need this package to manipulate data structues for data analysis and statistics\n",
    "df = pd.read_csv(\"MulticlassTextDataset.csv\", delimiter=',', dtype='unicode') \n",
    "#filename : give the full path to your file. if you file is in the same directory than your actual code, so just put the name of the file\n",
    "#delimiter: use the delimiter used in your file, for example here mine was \",\"\n",
    "\n",
    "# uploading a json file\n",
    "#import json\n",
    "#with open('TweetAnalysis.json') as json_file:\n",
    "    #data = json.load(json_file)\n",
    "# convert the json file to dataframe we can later manipulate\n",
    "#df = DataFrame(Data, columns= [\"tweet_id\", \"sentiment\", \"author\", \"content\"])\n",
    "#columns: are the names of the columns that you have in your json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['Consumer complaint narrative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-  visualisation of the data\n",
    "#give you the information about the file : number of rows and columns, data type of each and the memory storage\n",
    "df.info()\n",
    "#give you an overview of the 200 first rows\n",
    "df.head(200)\n",
    "#deleting the rows with no consumer complanit naratif \n",
    "#you totaly must delete the unnecessary rows so you won't have data type issues\n",
    "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
    "print(len(df['Consumer complaint narrative']))\n",
    "#give you the nomber of token (words, digit, ponctuation, ... ) splited using the blnak space\n",
    "df['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-  text pre-processing\n",
    "############# a- cleaning the data\n",
    "#text before preprocessing \n",
    "print(df['Consumer complaint narrative'][12000])\n",
    "#Delete the rows that have nothing in the relevant columns we gonna use : consumer complaint, company public response , product and issue\n",
    "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
    "df = df[pd.notnull(df['Product'])]\n",
    "df = df[pd.notnull(df['Issue'])]\n",
    "df = df[pd.notnull(df['Company public response'])]\n",
    "print(len(df['Consumer complaint narrative']))\n",
    "import nltk #world's most relevant package to text processing\n",
    "import re # package to regular expression to find , substract a substring from the text\n",
    "from nltk.corpus import stopwords #package of predefined stop words available in nltk\n",
    "from stop_words import get_stop_words # specified package of predefined stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # split the text into words/tokens\n",
    "\n",
    "########################################################################################################\n",
    "def cleaning_data (text) :\n",
    "    #this function process to the cleaning by using regular expression\n",
    "    #lover the text\n",
    "    text= text.lower()\n",
    "    #links\n",
    "    text=re.sub('https://\\S+','lien',text)\n",
    "    text=re.sub('http\\S+\\s','lien',text)\n",
    "    #email\n",
    "    text=re.sub('\\w+@\\w+','',text)\n",
    "    #ponctualtion\n",
    "    text=re.sub('[%s]' % re.escape(\"\"\"!\"#/\\$%&()*+',-.:;<=>?[]^_`{|}~°•\"\"\"), ' ', text)\n",
    "    text=re.sub('[%s]' % re.escape('\"\"'), ' ', text)\n",
    "    text=re.sub('[%s]' % re.escape(\"'\"), ' ', text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\d)+\",\" \",text) \n",
    "    #remove +2 blank spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #words with 1 to 2 letters\n",
    "    text= re.sub(r'\\W*\\b\\w{1,2}\\b', ' ', text)\n",
    "    #after visualiting the data we find that xxxx and xxx was used a lot , so we gonna just remove it \n",
    "    text= re.sub('xxx', ' ', text)\n",
    "    text= re.sub('xxxx', ' ', text)\n",
    "    #stop words\n",
    "    stopwords = get_stop_words('en') # stopwords from stopwords packages\n",
    "    text = ' '.join(word for word in text.split() if word not in (stopwords))\n",
    "                    \n",
    "    ####or you can use the nltk collection of stopwords\n",
    "    #set(stopwords.words('english'))\n",
    "    #stop_words = set(stopwords.words('english'))  \n",
    "    #text = ' '.join(word for word in text.split() if word not in (stop_words)\n",
    "    #non-relevant words               \n",
    "    nrelevant=['did']\n",
    "    words=word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word.lower() in nrelevant:\n",
    "            text=text.replace(word,' ')\n",
    "    #remove +2 blank spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "########################################################################################################\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x:cleaning_data(x))   \n",
    "print(df['Consumer complaint narrative'][12000])\n",
    "print(len(df['Consumer complaint narrative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning also the Company public response text data\n",
    "df['Company public response'] = df['Company public response'].apply(lambda x:cleaning_data(x)) \n",
    "print(len(df['Consumer complaint narrative']))\n",
    "#after preprocessing token count in the dataset (consumer complaint narrative)\n",
    "df['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-  text pre-processing\n",
    "############# b- data transformation\n",
    "freq = pd.Series(' '.join(df['Consumer complaint narrative']).split()).value_counts()[-50:] #select the less frequent words (with less than 55 apparition in the whole dataset) in the dataset\n",
    "freq = list(freq.index)\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "print(len(df['Consumer complaint narrative']))\n",
    "df['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, with this data, we are not gonna use the data transformation below. It's just an overview of what you can do in order to simplfy and unify you data in order to increase the accuracy of the futur model (if yiu have a looot of noise in you data for example, or you want to create a corpus/dictionnary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Spelling correction\n",
    "df['Consumer complaint narrative'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "print(df['Consumer complaint narrative'][12000])\n",
    "\n",
    "#tokenization using ngram\n",
    "words=TextBlob(df['Consumer complaint narrative'][12000]).words #with textblob\n",
    "words=word_tokenize(df['Consumer complaint narrative'][12000])  #with nltk\n",
    "tokens=nltk.ngrams(words,3)\n",
    "ngram_list=[]\n",
    "for ngram in tokens:\n",
    "    lowered_ngram_tokens = map(lambda token: token.lower(), ngram)\n",
    "    for token in lowered_ngram_tokens:\n",
    "        ngram_list.append(' '.join(ngram))\n",
    "print( ngram_list )\n",
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "df['Consumer complaint narrative'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "#or with nltk\n",
    "\n",
    "#Porter Stemming\n",
    "from nltk.stem import PorterStemmer \n",
    "lancaster=LancasterStemmer()\n",
    "def stemSentence(text):\n",
    "    porter = PorterStemmer() #create an object of class PorterStemmer\n",
    "    token_words=word_tokenize(text)\n",
    "    #token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x:stemSentence(x)) \n",
    "###########################\n",
    "\n",
    "#Lancaster Stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "def stemSentence(text):\n",
    "    lancaster=LancasterStemmer() #create an object of class LancasterStemmer\n",
    "    token_words=word_tokenize(text)\n",
    "    #token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(lancaster.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x:stemSentence(x)) \n",
    "###########################\n",
    "#for non english text I suggest to you to use the snowball stemmer \n",
    "#also available in english\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#frenchStemmer=SnowballStemmer(\"french\")  #example with a frenchstemming\n",
    "#Snowball Stemming\n",
    "def stemSentence(text):\n",
    "    englishStemmer=SnowballStemmer(\"english\") #create an object of class SnowballStemmer\n",
    "    token_words=word_tokenize(text)\n",
    "    #token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(englishStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x:stemSentence(x)) \n",
    "\n",
    "\n",
    "#lemmatisation\n",
    "from textblob import Word\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "df['Consumer complaint narrative'].head()\n",
    "#or with nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-  saving the data in a new file \n",
    "df.to_csv(\"after_preprocessing.csv\", sep=',', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-  extraction of relevant information\n",
    "#         URL EXTRACTION         \n",
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls(\"i have an issue while opening the link http://www.site.com/link1 and also the url to the form : www.site.com/form\")\n",
    "print(urls)  #return a list of url extracted from the string below\n",
    "###########################################\n",
    "#         Named Entity Recognition : NER : detecte places, dates, organization's name, money references, ...\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "text_processing = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "#article = nlp(ny_bb)\n",
    "len(text_processing.ents)\n",
    "labels = [x.label_ for x in text_processing.ents]\n",
    "print(text_processing.ents)# the entity detected from the text \n",
    "print(labels) #the label of each entity example: org, place, date, money , ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 spliting the data set to a 85% training data and 15% test data\n",
    "print(len(df['Consumer complaint narrative']))\n",
    "from sklearn.model_selection import train_test_split # split the data to train and test set\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "z= df['Issue']\n",
    "# spliting the data set to a 85% training data and 15% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 100)\n",
    "X_train\n",
    "\n",
    "####For the KNN model we need to process differently \n",
    "#preparation des données pour l'entrainement et le test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopwords=get_stop_words('en')\n",
    "vectorizer = TfidfVectorizer(stopwords) \n",
    "KNN_X = vectorizer.fit_transform(df['Consumer complaint narrative']) #we vectorize the data outside the model description\n",
    "KNN_y = df['Product']\n",
    "KNN_X_train, KNN_X_test, KNN_y_train, KNN_y_test = train_test_split(KNN_X, KNN_y, test_size=0.20, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-  Construction of the model : using tf-idf to the vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "############# a-  Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "ngram_range=(1,1)  #we gonna split the data into a unigram (word by word) before the vectorisation, to count It presence in the dataset\n",
    "nb = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB(alpha=1,n_jobs=-1)),\n",
    "              ])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# b-  Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression #one vs all\n",
    "logreg = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression( C=10,penalty ='l1', n_jobs=-1)), #n_jobs=-1 means that you will use all the process in your computer to do this job\n",
    "               ])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# c-  linear SVM\n",
    "from sklearn import svm # one vs one\n",
    "SVM = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', svm.SVC(kernel='linear', C = 1.0,n_jobs=-1)),\n",
    "               ])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# d-  SVM with SGD\n",
    "from sklearn.linear_model import SGDClassifier #one vs rest\n",
    "sgd = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='log',n_jobs=-1, penalty='l2',alpha=1e-6, random_state = 200, max_iter =150)),\n",
    "               ])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# e-  Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomf= Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=20, criterion='entropy',n_jobs=-1))\n",
    "                  ])\n",
    "%time\n",
    "#n_estimators: number of trees, by default is 10\n",
    "#creterion: measure the quality of a split, by default it \"gini\" impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# f-  K nearest nighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Clustering the document with KNN classifier\n",
    "#we use the metric of distance to weights the neighbors\n",
    "#we consider the 4 nearest neighbors to check in which cluster/class the data lean on\n",
    "modelknn = KNeighborsClassifier(n_neighbors=4, n_jobs=-1, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-  Construction of the model : by selecting the K best features in the corpus to vectorize the text, using the chi2 method\n",
    "#Example on the naive bayes model\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "#Naive Bayes classifier for multinomial models\n",
    "ngram_range=(1,1)\n",
    "nb = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range)),\n",
    "               ('chi',  SelectKBest(chi2, k=20000)), #we are going to select the 20000 best features (most referenced in the whole dataset) to describe and so vectorize the text\n",
    "               ('clf', MultinomialNB(alpha=1)), \n",
    "              ])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# a-  Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#my_tag represent the label that specify every single group/class in your dataset. Here for example is the type of the product\n",
    "my_tags=[\"Bank account or service\",\"Checking or savings account\",\"Consumer Loan\",\"Credit card\",\"Credit card or prepaid card\",\"Credit reporting\",\"Credit reporting, credit repair services, or other personal consumer reports\",\"Debt collection\",\"Money transfer, virtual currency, or money service\",\"Money transfers\",\"Mortgage\",\"Other financial service\",\"Payday loan\",\"Payday loan, title loan, or personal loan\",\"Prepaid card\",\"Student loan\",\"Vehicle loan or lease\",\"Virtual currency\"]\n",
    "############# a-  Naive Bayes\n",
    "y_pred = nb.predict(X_test)\n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# b-  Logistic regression\n",
    "logreg.fit(X_train, y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "############# b-  Logistic regression\n",
    "y_pred = logreg.predict(X_test)\n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# c-  linear SVM\n",
    "SVM.fit(X_train, y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "############# c-  linear SVM\n",
    "y_pred = SVM.predict(X_test)\n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# d-  SVM with SGD\n",
    "sgd.fit(X_train, y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "############# d-  SVM with SGD\n",
    "y_pred = sgd.predict(X_test)\n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# e-  Random forest\n",
    "randomf.fit(X_train, y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "############# e-  Random forest \n",
    "y_pred = randomf.predict(X_test)\n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8-  training the model\n",
    "############# f-  K nearest nighbors\n",
    "modelknn.fit(KNN_X_train,KNN_y_train)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9-  test & evaluation ofthe model\n",
    "############# f-  K nearest nighbors\n",
    "KNN_y_pred = modelknn.predict(KNN_X_test) \n",
    "%time\n",
    "print('accuracy %s' % accuracy_score(KNN_y_pred, KNN_y_test))\n",
    "print(classification_report(KNN_y_test, KNN_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-  saving the model\n",
    "\n",
    "from sklearn import model_selection\n",
    "import pickle\n",
    "with open('svm_classifier', 'wb') as picklefile:  \n",
    "    pickle.dump(SVM,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-  tuning the parameters of the model\n",
    "\n",
    "############ a- NB- LR- RF- SVM\n",
    "# the example below for the svm classifier, can apply to any of the other classifier, for any of their parameters\n",
    "# gridsearch will build, test and evaluate each model from the other and will give up the appropriate one with the specific parameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2),(1,3),(2,2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "              'clf__C': (1e1,1.0),\n",
    "              \n",
    "             }\n",
    "gs_clf = GridSearchCV(SVM, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11- tuning the parameters of the model\n",
    "############ B- KNN\n",
    "#as the main parameter that import in the KNN model is the number of neighbors that we consider in the model\n",
    "#the paremter tuning will be to find the best one with the best accuracy\n",
    "\n",
    "error = []\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 40):  #you can obviously change the range you wanna test \n",
    "    print(i)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=-1, weights='distance') # we build the model , adding the other parameters\n",
    "    knn.fit(KNN_X_train, KNN_y_train) # train it \n",
    "    KNN_pred_i = knn.predict(KNN_X_test) #test it\n",
    "    error.append(np.mean(KNN_pred_i != KNN_y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we will use a graph to show the evolution of the error while the number of neighbors rise\n",
    "plt.figure(figsize=(20, 20))  \n",
    "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')  \n",
    "plt.xlabel('K Value')  \n",
    "plt.ylabel('Mean Error') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
